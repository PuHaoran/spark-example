{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核心概念\n",
    "spark可以分为一个driver（笔记本电脑或集群网关机器）和若干个executor（各个节点），通过SparkContext连接Spark集群、创建RDD、累加器（accumlator）、广播变量（broadcast variables），简单认为SparkContext是Spark程序的根本。\n",
    "\n",
    "<img src=\"../imgs/img01.png\" width = \"250\" height = \"250\" />\n",
    "\n",
    "在Spark中所有的处理和计算任务都被组织成一系列Resilient Distributed Dataset（弹性分布式数据集，简称RDD）上的transformations（转换）和actions（动作）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化RDD方法1\n",
    "\n",
    "如果你本地内存中已经有一份序列数据（python中的list），你可以通过sc.parallelize去初始化一个RDD。当你执行这个操作时，list中的元素会被自动分块（partitioned），并把每一块送到不同的机器上。\n",
    "\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext('local', 'pyspark')\n",
    "    \n",
    "    rdd = sc.parallelize([1,2,3,4,5])\n",
    "    rdd\n",
    "    \n",
    "    # 查看在多少个分区\n",
    "    rdd.getNumPartitions()\n",
    "    \n",
    "    # 查看分区状况\n",
    "    rdd.glom().collect()\n",
    "    \n",
    "    # tips：使用sc.parallelize，可以把list，numpy array或pandas series或Pandas Dataframe转成Spark RDD。\n",
    "   \n",
    "### 初始化RDD方法2\n",
    "第2种方法当然是直接读文本到RDD中。\n",
    "文本中的每一行都被当做item，需要注意的是Spark一般都默认你的路径指向HDFS，若要从本地读取文件的话，需要使用file://开头的全局路径。\n",
    "    \n",
    "    import os\n",
    "    cwd = os.getcwd()\n",
    "    cwd\n",
    "    rdd = sc.textFile(\"file://\" + cwd + \"/Desktop/spark-example/data/names/yob1880.txt\")\n",
    "    rdd\n",
    "    rdd.first()\n",
    "    \n",
    "其余初始化RDD的方法\n",
    "    \n",
    "    1. HDFS上的文件\n",
    "    2. Hive中的数据库和表\n",
    "    3. Spark SQL得到的结果\n",
    "    \n",
    "#### 练习作业\n",
    "把names/yob1880.txt文件读成RDD格式的对象，命名成rdd，取出第一行（tips：rdd.first()) 统计行数（tips：rdd.count()）\n",
    "    \n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext('local', 'pyspark')\n",
    "    rdd = sc.textFile(\"file://\" + cwd + \"/Desktop/spark-example/data/names/yob1880.txt\")\n",
    "    rdd.first()\n",
    "    rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD transformation\n",
    "RDD可以经过一系列变化得到新的RDD，RDD上最常用到transformation：\n",
    "    \n",
    "    map() 对RDD的每一个item都执行同一个操作\n",
    "    flatMap() 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把所有的结果组成新的list\n",
    "    filter() 筛选出来满足条件的item\n",
    "    distinct() 对RDD中的item去重\n",
    "    sample() 从RDD中的item中采样一部分，有放回或无放回\n",
    "    sortBy() 对RDD中的item进行排序\n",
    "   \n",
    "    ps: collect()的action可以把item转化为list，Transformation，可以一个接一个地串联。\n",
    "    \n",
    "    def odd_opt(x):\n",
    "        if x % 2 == 1:\n",
    "            return 2 * x\n",
    "        else:\n",
    "            return x\n",
    "            \n",
    "    result_rdd = (rdd.map(odd_opt).filter(lambda x: x > 6).distinct())\n",
    "    result_rdd.collect()\n",
    "    \n",
    "#### 练习作业\n",
    "\n",
    "使用`sc.textFile()`载入`\"names/yob1880.txt\"`到RDD中<br>\n",
    "练习`map`, `filter` 和 `distinct`操作<br>\n",
    "进行组合操作得到所有以M开头的名字组成的RDD<br>\n",
    "tips：`first()` 这个action也许可以在中间的某些部分帮助到你\n",
    "\n",
    "rdd.filter(lambda x: x[0] == 'M').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD间的操作\n",
    "\n",
    "    rdd1.union(rdd2) 所有rdd1和rdd2中的item组合\n",
    "    rdd1.intersection(rdd2) rdd1和rdd2的交集\n",
    "    rdd1.substract(rdd2) 所有在rdd1但不在rdd2中的item（差集）\n",
    "    rdd1.cartesian(rdd2) rdd1和rdd2中所有元素的笛卡尔乘积\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 惰性计算\n",
    "\n",
    "Spark中的一个核心概念是惰性计算。当把一个RDD转换为另一个RDD的时候，这个转换不会立即生效。\n",
    "\n",
    "Spark会把它先记在心里，等到真的需要拿到转换结果的时候，才会重新组织你的transformations。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更复杂的transform和action\n",
    "以元组形式组织的k-v对(key, value)，我们把它叫做pair RDDs，这种item结构的数据，定义了一系列的transform和action。\n",
    "\n",
    "    reduceByKey() 对所有有着相同key的items执行reduce操作\n",
    "    groupByKey() 返回类似(key, listOfValues)元组的RDD，后面的value list是同一个key下面的\n",
    "    sortByKey() 按照key排序\n",
    "    countByKey() 按照key去对item个数进行统计\n",
    "    collectAsMap() 和collect有些类似，但返回的是k-v字典\n",
    "    \n",
    "下面是spark的一些例子，如何使用spark如何做统计？\n",
    "\n",
    "    rdd = sc.parallelize([\"Hello hello\", \"Hello New Bob\", \"York says hello\"])\n",
    "    result_rdd = (rdd.flatMap(lambda x: x.split(\" \")).map(lambda word: word.lower()).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y))\n",
    "    result_rdd.collect()\n",
    "    \n",
    "    [('hello', 4), ('new', 1), ('bob', 1), ('york', 1), ('says', 1)]\n",
    "    \n",
    "    我们将结果以k-v字典形式返回\n",
    "    result = result_rdd.collectAsMap()\n",
    "    result\n",
    "    \n",
    "    {'hello': 4, 'new': 1, 'bob': 1, 'york': 1, 'says': 1}\n",
    "\n",
    "    如果你想要出现频次最高的2个词，可以这样做：\n",
    "    \n",
    "    result_rdd.sortBy(keyfunc=lambda(word, count): count, ascending=False).take(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定2个RDD后，可以通过一个类似SQL的方式join他们。\n",
    "\n",
    "    ##### Home of different people\n",
    "    homesRDD = sc.parallelize([\n",
    "            ('Brussels', 'John'),\n",
    "            ('Brussels', 'Jack'),\n",
    "            ('Leuven', 'Jane'),\n",
    "            ('Antwerp', 'Jill'),\n",
    "        ])\n",
    "\n",
    "    ##### Quality of life index for various cities\n",
    "    lifeQualityRDD = sc.parallelize([\n",
    "            ('Brussels', 10),\n",
    "            ('Antwerp', 7),\n",
    "            ('RestOfFlanders', 5),\n",
    "        ])\n",
    "\n",
    "    homesRDD.join(lifeQualityRDD).collect()\n",
    "    \n",
    "    homesRDD.leftOuterJoin(lifeQualityRDD).collect()\n",
    "    \n",
    "    homesRDD.rightOuterJoin(lifeQualityRDD).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "Spark SQL是架在Spark core之上允许我们用SQL去操作大型数据库的高级层\n",
    "\n",
    "Spark SQL里有一种新RDD，叫做SchemaRDD，其实你可以把他看做一个table，SchemaRDD类似一种图表的RDD，每个item都是一个Row。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>练习作业</font><br>\n",
    "**回顾一下，我们可以在pandas里定义这样一个dataframe**\n",
    "```\n",
    "data = {\n",
    "    'country': ['BE', 'BE', 'BE', 'NL', 'NL', 'NL'],\n",
    "    'year': [1913, 1950, 2003, 1913, 1950, 2003],\n",
    "    'gdp_per_capita': [4220, 5462, 21205, 4049, 5996, 21480]\n",
    "}\n",
    "frame = DataFrame(data)\n",
    "```\n",
    "**请大家在Spark中生成一个SchemaRDD，再做一个简单的SQL分析：求这3年各首都的平均GDP**\n",
    "\n",
    "from pyspark import SQLContext, Row\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "rdd = sc.parallelize([Row(country='BE', year=1913, gdp_per_capita=4220), Row(country='BE', year=1950, gdp_per_capita=5462), Row(country='BE', year=2003, gdp_per_capita=21205), Row(country='NL', year=1913, gdp_per_capita=4049), Row(country='NL', year=1950, gdp_per_capita=5996), Row(country='NL', year=2003, gdp_per_capita=21480)])\n",
    "\n",
    "schemaRDD = sqlCtx.createDataFrame(rdd)\n",
    "\n",
    "schemaRDD.registerTempTable(\"country\")\n",
    "\n",
    "avgGDP = sqlCtx.sql(\"select country, AVG(gdp_per_capita) as gdp from country group by country\")\n",
    "\n",
    "avgGDP.collect()\n",
    "\n",
    "[Row(country='NL', gdp=10508.333333333334), Row(country='BE', gdp=10295.666666666666)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习作业\n",
    "\n",
    "sc.textFile, union 和 map练习。 把提供的names数据集构建成一个SchemaRDD，包含year, name, sex和births，把它注册成一个表\"names\"\n",
    "\n",
    "1.求从1939到1945年美国总出生的人口\n",
    "\n",
    "2.统计从1880到2014每一年叫“Mary”的宝宝出生数目，并用matplotlib绘制成图像。\n",
    "\n",
    "3.统计从1880到2014每一年男孩和女生的出生数，并绘制在一张图中。\n",
    "\n",
    "4.统计出来每一年出生的宝宝频次最高的前1000个名字，以及它们的占比，并用绘图方式去展示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SQLContext, Row \n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "files = os.listdir('/Users/hammer/Desktop/spark-example/data/names/')\n",
    "for f in files:\n",
    "    if f == 'yob1880.txt':\n",
    "        y = int(f[3:7])\n",
    "        text_file_path = '/Users/hammer/Desktop/spark-example/data/names/' + f \n",
    "        print(text_file_path)\n",
    "        lines = sc.textFile(text_file_path)\n",
    "        \n",
    "        parts = lines.map(lambda l: l.split(\",\"))\n",
    "        people1 = parts.map(lambda p: Row(year=y, name=p[0], sex=p[1], births=p[2]))\n",
    "    elif f[0] != '.':\n",
    "        y = int(f[3:7])\n",
    "        text_file_path = '/Users/hammer/Desktop/spark-example/data/names/' + f \n",
    "        print(text_file_path)\n",
    "        lines = sc.textFile(text_file_path)\n",
    "        \n",
    "        parts = lines.map(lambda l: l.split(\",\"))\n",
    "        people2 = parts.map(lambda p: Row(year=y, name=p[0], sex=p[1], births=p[2]))\n",
    "        people1 = people1.union(people2)\n",
    "        \n",
    "schemaRDD = sqlCtx.createDataFrame(people1)\n",
    "\n",
    "schemaRDD.registerTempTable(\"names\")\n",
    "#print(people1.count(), people2.count())\n",
    "\n",
    "# 1.求从1939到1945年美国总出生的人口\n",
    "cnt = sqlCtx.sql(\"select Count(name) as cnt from names where year >= 1939 and year <= 1945\")\n",
    "cnt.collect() # [Row(cnt=63973)] \n",
    "\n",
    "# 2.统计从1880到2014每一年叫“Mary”的宝宝出生数目，并用matplotlib绘制成图像。\n",
    "Mary_cnt = sqlCtx.sql(\"select Count(name) as mary_cnt from names where year >= 1880 and year <= 2014 and name = 'Mary' group by year \")\n",
    "Mary_cnt.collect()\n",
    "\n",
    "# 3.统计从1880到2014每一年男孩和女生的出生数\n",
    "Mary_F_cnt = sqlCtx.sql(\"select Count(name) as name_cnt from names where sex='F' and year >= 1880 and year <= 2014 group by year\")\n",
    "Mary_M_cnt = sqlCtx.sql(\"select Count(name) as name_cnt from names where sex='M' and year >= 1880 and year <= 2014 group by year\")\n",
    "\n",
    "# 4.统计出来每一年出生的宝宝频次最高的前1000个名字\n",
    "Top_1000 = sqlCtx.sql(\"select name, Count(name) as name_cnt from names where year >= 1880 and year <= 2014 group by year, name order by name_cnt desc  limit 1000\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
